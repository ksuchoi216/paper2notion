{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f9c5fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow \n",
    "\n",
    "from pprint import pprint\n",
    "from IPython.display import display\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a373dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'system_prompt_name': 'papar2notion_system', 'system_prompt_num': 1, 'human_prompt_name': 'papar2notion_human', 'human_prompt_num': 1}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def load_config(config_path=\"./configs/config.yaml\"):\n",
    "    with open(config_path, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "config = load_config()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54620702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:30001\n"
     ]
    }
   ],
   "source": [
    "url = f'{os.getenv(\"MLFLOW_TRACKING_URI\")}'\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "007c9a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment exists, ID: 1\n",
      "🏃 View run adventurous-yak-763 at: http://localhost:30001/#/experiments/0/runs/4624360a22194923a6339797928ca294\n",
      "🧪 View experiment at: http://localhost:30001/#/experiments/0\n",
      "✅ 연결 및 로그 완료\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "uri = mlflow.get_tracking_uri()\n",
    "\n",
    "# 실험 가져오기 또는 생성\n",
    "exp = mlflow.get_experiment_by_name(\"test-connection\")\n",
    "if exp is None:\n",
    "    exp_id = mlflow.create_experiment(\"test-connection\")\n",
    "    print(\"Created experiment ID:\", exp_id)\n",
    "else:\n",
    "    print(\"Experiment exists, ID:\", exp.experiment_id)\n",
    "\n",
    "# 테스트 로그\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"test_param\", \"ok\")\n",
    "print(\"✅ 연결 및 로그 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c51f4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import google.generativeai as genai\n",
    "\n",
    "# # Gemini API 키 입력\n",
    "\n",
    "\n",
    "# def list_gemini_models():\n",
    "#     \"\"\"\n",
    "#     Google Gemini API로 사용 가능한 모델 이름 출력\n",
    "#     \"\"\"\n",
    "#     # API 키로 인증\n",
    "#     genai.configure()\n",
    "\n",
    "#     # 사용 가능한 모델 목록 가져오기\n",
    "#     models = genai.list_models()\n",
    "\n",
    "#     print(\"📦 사용 가능한 Gemini 모델 목록:\")\n",
    "#     for model in models:\n",
    "#         print(f\"- {model.name}\")\n",
    "\n",
    "\n",
    "# # 실행\n",
    "# if __name__ == \"__main__\":\n",
    "#     list_gemini_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "493b431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def create_llm(model_type: str = \"gemini\", **kwargs):\n",
    "    \"\"\"\n",
    "    LLM 인스턴스 생성\n",
    "\n",
    "    Args:\n",
    "        model_type: \"gemini\" 또는 \"openai\"\n",
    "        **kwargs: 모델별 추가 설정\n",
    "    \"\"\"\n",
    "    if model_type.lower() == \"gemini\":\n",
    "        print(\"Using Google Gemini model\")\n",
    "        return ChatGoogleGenerativeAI(\n",
    "            model=kwargs.get(\"model\", \"gemini-2.5-pro\"),\n",
    "            temperature=kwargs.get(\"temperature\", 0.0),\n",
    "        )\n",
    "\n",
    "    elif model_type.lower() == \"openai\":\n",
    "        print(\"Using OpenAI model\")\n",
    "        return ChatOpenAI(\n",
    "            model=kwargs.get(\"model\", \"gpt-4o-mini\"),\n",
    "            temperature=kwargs.get(\"temperature\", 0.0),\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"지원하지 않는 모델 타입: {model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5789d34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Google Gemini model\n"
     ]
    }
   ],
   "source": [
    "llm = create_llm(\"gemini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947d7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model type: gemini\n",
      "System Prompt 4:\n",
      "## System\n",
      "너는 설명을 친절하게 잘해주는 교수야. \n",
      "논문을 아래와 같이 요약해서 배경지식이 없는 학생들에게 설명한다고 생각해.\n",
      "논문을 읽기 전 미리 배경지식을 얻을 수 있는 요약을 만들어줘.\n",
      "반드시 content 주요 내용 사이에는 \"---\"을 삽입해서 명확히 구분해줘.\n",
      "\n",
      "## Content 주요 내용\n",
      "아래와 같이 9가지 부분으로 요약해줘.\n",
      "1. 연구 동기(왜 이 연구를 했는지)와 논문 제목의 이유.\n",
      "2. 요약. 이 부분은 Abstract과 Conclusion 부분의 내용을 요약.\n",
      "    요약할 때는 왜 이 연구를 했는지(Why), 그래서 어떤 연구를 제시했는지(What), 어떻게 적용 했는지(How)로 설명해줘.\n",
      "3. 논문의 methodology. 직관적인 예시를 통해 자세한 설명 부탁해. \n",
      "4. 논문에서 사용한 데이터 셋(dataset)과 평가 지표(evaluation metric)에 대해 설명해줘.\n",
      "    - 데이터 셋과 평가지표는 본문 내용을 전부 알려줘.\n",
      "5. 결과에 대해 간략히 설명해줘.\n",
      "6. 논문이 제시한 방법의 장단점.\n",
      "7. 이 논문에서 나온 추천할 만한 References. 예를 들어, Reference를 직접적으로 발전 시켜 연구한 논문이면, reference를 부탁해.\n",
      "8. 아래 정보도 개조식으로 정리해줘. 각 요소마다 복사하기 편하게 정리해줘. 예를 들어, title 따로(title만), year 따로(YYYY 숫자만), citation 따로(숫자만), tags 따로(tag1, tag2, tag3, tag4)\n",
      "- title: 논문 이름\n",
      "- year: 발행년도\n",
      "- citation: 인용수\n",
      "- tags: 논문을 분류하기 위해 필요한 키워드들, 큰 카테고리 및 세부 기술의 키워드가 필요함.\n",
      "9. short_summary: 최종적으로 전체적인 내용을 핵심적인 2개 문장을 개조식으로 요약해줘. 반드시 천천히 생각해보고 차근차근 생각해서 핵심적인 단어와 표현으로 부탁해. 제일 중요한 부분이야. 복사하기 편하게 정리해줘.\n",
      "Human Prompt 2:\n",
      "아래 링크의 논문을 요약해줘. 반드시 아래 instructions을 지켜서 대답해줘. 논문을 천천히 이해하고 대답해줘.\n",
      "<link>\n",
      "{paper_url}\n",
      "</link>\n",
      "\n",
      "<instructions>\n",
      "아래와 같은 지시사항을 지켜주며 작성해줘.\n",
      "- 각 부분은 최대 8문장까지 만들어줘. 단, 너무 짧으면 안돼. 단, methodology 부분은 자세하고 길게 설명해줘.\n",
      "- 기술적 중요 사항은 체계적으로 쉽게 설명해줘.\n",
      "- 잘 모르겠다면, 잘 모른다고 대답해줘.\n",
      "- content 주요 내용 사이에는 '---'로 구분해줘.\n",
      "- 핵심 2문장 요약은 개조식으로 부탁해. \n",
      "</instructions>\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "import datetime\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    title: str = Field(..., description=\"논문의 제목\")\n",
    "    year: int = Field(..., description=\"논문의 출판 연도\")\n",
    "    citation: int = Field(..., description=\"논문의 인용 수\")\n",
    "    short_summary: str = Field(..., description=\"논문의 요약\")\n",
    "    tags: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"논문의 키워드. 키워드는 논문을 분류하기 위해 필요한 정보. 대분류부터 디테일한 키워드까지 포함. 최대 4개.\",\n",
    "    )\n",
    "    content: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"논문의 주요 내용. Content의 주요 내용으로 system prompt에서 지시한 내용의 답변입니다.\",\n",
    "    )\n",
    "    paper_url: str = Field(..., description=\"논문의 URL\")\n",
    "\n",
    "\n",
    "class LLMChain:\n",
    "    def __init__(\n",
    "        self,\n",
    "        experiment_name: str = \"paper2notion\",\n",
    "        model_type: str = \"gemini\",\n",
    "        prompt_name: str = \"paper2notion\",\n",
    "        system_prompt_num: int = 1,\n",
    "        human_prompt_num: int = 1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        print(f\"model type: {model_type}\")\n",
    "        self.llm = self.create_llm(model_type, **kwargs)\n",
    "        self.system_prompt, self.human_prompt = self.get_prompt(\n",
    "            prompt_name=prompt_name,\n",
    "            system_prompt_num=system_prompt_num,\n",
    "            human_prompt_num=human_prompt_num,\n",
    "        )\n",
    "        self.experiment_id, self.run_name = self.create_run_name(\n",
    "            experiment_name, model_type, system_prompt_num, human_prompt_num\n",
    "        )\n",
    "        self.__setup__(self.llm, self.system_prompt, self.human_prompt)\n",
    "\n",
    "    def create_run_name(\n",
    "        self, experiment_name, model_type, system_prompt_num, human_prompt_num\n",
    "    ):\n",
    "        if experiment := mlflow.get_experiment_by_name(experiment_name):\n",
    "            experiment_id = experiment.experiment_id\n",
    "        else:\n",
    "            experiment_id = mlflow.create_experiment(experiment_name)\n",
    "        run_name = datetime.datetime.now().strftime(\"%m%d-%H%M%S\")\n",
    "        run_name += f\"_{model_type}_s{system_prompt_num}_h{human_prompt_num}\"\n",
    "        return experiment_id, run_name\n",
    "\n",
    "    def create_llm(self, model_type: str = \"gemini\", **kwargs):\n",
    "        if model_type.lower() == \"gemini\":\n",
    "            return ChatGoogleGenerativeAI(\n",
    "                model=kwargs.get(\"model\", \"gemini-2.5-pro\"),\n",
    "                temperature=kwargs.get(\"temperature\", 0.0),\n",
    "            )\n",
    "        elif model_type.lower() == \"openai\":\n",
    "            return ChatOpenAI(\n",
    "                model=kwargs.get(\"model\", \"gpt-4o-mini\"),\n",
    "                temperature=kwargs.get(\"temperature\", 0.0),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"지원하지 않는 모델 타입: {model_type}\")\n",
    "\n",
    "    def get_prompt(self, prompt_name, system_prompt_num=1, human_prompt_num=1):\n",
    "        system_prompt = mlflow.genai.load_prompt(  # type: ignore\n",
    "            f\"prompts:/paper2notion_system/{system_prompt_num}\"\n",
    "        ).to_single_brace_format()\n",
    "        human_prompt = mlflow.genai.load_prompt(  # type: ignore\n",
    "            f\"prompts:/paper2notion_human/{human_prompt_num}\"\n",
    "        ).to_single_brace_format()\n",
    "\n",
    "        print(f\"System Prompt {system_prompt_num}:\")\n",
    "        print(system_prompt)\n",
    "\n",
    "        print(f\"Human Prompt {human_prompt_num}:\")\n",
    "        print(human_prompt)\n",
    "\n",
    "        return system_prompt, human_prompt\n",
    "\n",
    "    def __setup__(self, llm, system_prompt, human_prompt):\n",
    "        parser = PydanticOutputParser(pydantic_object=Output)\n",
    "        format_instructions = (\n",
    "            parser.get_format_instructions().replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "        )\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [(\"system\", system_prompt), (\"human\", human_prompt + format_instructions)]\n",
    "        )\n",
    "\n",
    "        self.chain = prompt | llm | parser\n",
    "\n",
    "    def run(self, paper_url):\n",
    "        input_dict = {\"paper_url\": paper_url}\n",
    "\n",
    "        with mlflow.start_run(experiment_id=self.experiment_id, run_name=self.run_name):\n",
    "            mlflow.log_param(\"paper_url\", paper_url)\n",
    "            output = self.chain.invoke(input_dict)\n",
    "            mlflow.log_params(output.dict())\n",
    "            return output\n",
    "\n",
    "\n",
    "chain = LLMChain(\n",
    "    experiment_name=\"paper2notion\",\n",
    "    model_type=\"gemini\",\n",
    "    system_prompt_num=4,\n",
    "    human_prompt_num=2,\n",
    ")\n",
    "\n",
    "output = chain.run(\"https://arxiv.org/pdf/2305.07895\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca7c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"1. 연구 동기(왜 이 연구를 했는지)와 논문 제목의 이유.\\n안녕하세요, 학생 여러분. 오늘 다룰 논문은 LLM(거대 언어 모델)이 어떻게 스스로 더 똑똑해질 수 있는지에 대한 흥미로운 연구입니다. 우리가 글을 쓸 때 초고를 작성하고, 다시 읽어보며 어색한 부분을 고치는 것처럼, LLM도 한 번에 완벽한 결과물을 내놓기는 어렵습니다. 이 연구는 바로 이 지점에서 출발합니다. LLM이 생성한 결과물을 스스로 평가하고, 그 피드백을 바탕으로 더 나은 결과물을 만들도록 할 수는 없을까? 라는 질문이 연구의 동기입니다. 논문 제목인 'Self-Refine'은 이 과정을 아주 직관적으로 보여줍니다. 모델 스스로(Self) 자신의 결과물을 개선(Refine)한다는 의미를 담고 있죠. 외부의 도움 없이 스스로 발전하는 똑똑한 모델을 만들고자 한 것입니다.\", \"2. 요약. 이 부분은 Abstract과 Conclusion 부분의 내용을 요약.\\n이 논문을 '왜, 무엇을, 어떻게'로 요약해 보겠습니다. (Why) 왜 이 연구를 했을까요? 앞서 말했듯, GPT와 같은 LLM은 한 번의 시도로는 복잡한 문제에 대해 부정확하거나 미흡한 답변을 내놓는 경우가 많기 때문입니다. (What) 그래서 어떤 연구를 제시했나요? 연구진은 'Self-Refine'이라는 간단하지만 강력한 프레임워크를 제안했습니다. 이는 LLM이 자신의 결과물을 비평하고, 그 비평을 바탕으로 스스로 결과물을 수정해 나가는 반복적인 과정입니다. (How) 어떻게 적용했을까요? 별도의 모델 훈련이나 데이터 없이, 이미 존재하는 LLM(예: GPT-3.5, GPT-4)에게 특정 역할을 부여하는 프롬프트(지시어)만으로 이 과정을 구현했습니다. 코딩, 작문, 수학 문제 풀이 등 다양한 작업에 적용하여 Self-Refine이 기존의 단일 답변 방식보다 훨씬 뛰어난 성능을 보임을 입증했습니다.\", '3. 논문의 methodology. 직관적인 예시를 통해 자세한 설명 부탁해.\\nSelf-Refine의 방법론은 우리가 과제를 하는 과정과 아주 흡사해서 이해하기 쉽습니다. 총 3단계의 반복 과정으로 이루어집니다.\\n\\n1단계: **초안 생성 (Initial Output)**\\n먼저, 모델에게 과제를 줍니다. 예를 들어, \\'코끼리를 냉장고에 넣는 법을 창의적으로 설명해줘\\'라는 프롬프트를 입력합니다. 그럼 모델은 첫 번째 답변, 즉 초안을 생성합니다. \"1. 냉장고 문을 연다. 2. 코끼리를 넣는다. 3. 냉장고 문을 닫는다.\" 와 같은 고전적인 답변을 내놓았다고 가정해 봅시다.\\n\\n2단계: **자가 피드백 (Self-Feedback)**\\n이제 모델에게 역할을 바꿔서 비평가가 되어보라고 지시합니다. 방금 생성한 초안을 보여주며 \"이 답변의 문제점은 무엇이고, 어떻게 개선하면 더 창의적일까?\"라고 묻습니다. 그러면 모델은 스스로를 평가하며 \"이 답변은 너무 고전적이고 재미가 없습니다. 코끼리의 크기나 냉장고의 종류 같은 물리적 제약을 무시하는 상상력이 더 필요합니다. 예를 들어, \\'코끼리를 데이터화해서 이메일로 보낸 뒤 냉장고 모양의 USB에 저장한다\\' 와 같은 아이디어를 추가하면 좋겠습니다.\" 라는 피드백을 생성합니다.\\n\\n3단계: **개선 (Refine)**\\n마지막으로, 모델에게 원래의 과제와 초안, 그리고 방금 생성한 피드백을 모두 함께 줍니다. 그리고 이렇게 지시합니다. \"이 피드백을 바탕으로 원래의 답변을 더 창의적으로 수정해줘.\" 그러면 모델은 피드백을 참고하여 \"코끼리를 \\'코끼리.zip\\' 파일로 압축한 뒤, 클라우드에 업로드합니다. 그리고 스마트 냉장고의 앱 스토어에서 해당 파일을 다운로드하여 설치하면 됩니다.\" 와 같이 훨씬 더 창의적이고 개선된 결과물을 내놓게 됩니다.\\n\\n이 3단계 과정을 여러 번 반복하면(Iteration), 결과물은 점점 더 정교하고 완성도 높게 발전합니다. 이 모든 과정이 외부의 개입 없이 오직 모델 스스로의 능력만으로 이루어진다는 점이 이 방법론의 핵심입니다.', '4. 논문에서 사용한 데이터 셋(dataset)과 평가 지표(evaluation metric)에 대해 설명해줘.\\n이 연구는 Self-Refine의 효과를 다양한 분야에서 검증하기 위해 총 7개의 데이터셋을 사용했습니다.\\n\\n**데이터셋(Datasets):**\\n*   **코드 생성/개선 (Code Generation/Improvement):** `HumanEval-Plus`, `MBPP-Plus` - 파이썬 코딩 문제를 푸는 데이터셋입니다.\\n*   **수학 추론 (Mathematical Reasoning):** `GSM8K` - 초등학교 수준의 수학 응용 문제입니다.\\n*   **상식 기반 문장 생성 (Commonsense Generation):** `CommonGen` - 주어진 몇 개의 단어를 사용해 상식적인 문장을 만드는 데이터셋입니다.\\n*   **사회적 상식 추론 (Social Commonsense Reasoning):** `SocialIQA` - 사회적 상황에 대한 질문에 답하는 데이터셋입니다.\\n*   **대화 응답 생성 (Dialogue Response Generation):** `Topical-Chat` - 주어진 주제에 대해 대화를 이어나가는 데이터셋입니다.\\n*   **두문자어 생성 (Acronym Generation):** `BIG-Bench`의 일부로, 주어진 구절에 맞는 두문자어를 생성하는 과제입니다.\\n\\n**평가 지표(Evaluation Metrics):**\\n*   코드 생성 문제(`HumanEval-Plus`, `MBPP-Plus`)에서는 생성된 코드가 테스트 케이스를 통과하는지 여부를 측정하는 `pass@1`을 사용했습니다.\\n*   수학 문제(`GSM8K`)와 사회적 상식 추론(`SocialIQA`)에서는 정답을 맞혔는지 `정확도(Accuracy)`를 측정했습니다.\\n*   두문자어 생성에서는 생성된 단어가 정답과 정확히 일치하는지 `Exact Match(EM)`로 평가했습니다.\\n*   문장 생성(`CommonGen`)에서는 생성된 문장이 얼마나 자연스러운지를 평가하는 `BLEU`, `SPICE`, `CIDEr` 같은 자동 평가 지표를 사용했습니다.\\n*   대화 응답(`Topical-Chat`)과 같이 정답이 정해져 있지 않은 과제에서는, GPT-4를 평가자로 활용하여 응답의 품질(일관성, 흥미도 등)을 1~5점 척도로 평가하는 방식을 도입했고, 사람도 직접 평가에 참여했습니다.', '5. 결과에 대해 간략히 설명해줘.\\n결과는 매우 인상적이었습니다. Self-Refine을 적용했을 때, GPT-3.5와 GPT-4 같은 모델의 성능이 실험에 사용된 7개 과제 모두에서 눈에 띄게 향상되었습니다. 특히, 기존의 한 번에 답변을 생성하는 방식(one-shot)에 비해 절대 성능이 평균 20% 가까이 오르는 큰 폭의 개선을 보였습니다. 재미있는 점은, 이러한 자기 개선 능력은 모델의 크기가 클수록 더 잘 발현된다는 것입니다. 작은 모델들은 유용한 피드백을 생성하지 못해 성능 향상이 미미했습니다. 또한, 개선 과정을 반복할수록 성능이 계속 오르다가 특정 지점에서 수렴하는 경향을 보였습니다. 이는 여러 번 고쳐 쓸수록 글이 좋아지는 것과 같은 이치라고 할 수 있습니다.', \"6. 논문이 제시한 방법의 장단점.\\n이 방법은 여러 장점이 있지만, 명확한 한계도 존재합니다.\\n\\n**장점:**\\n*   **범용성:** 별도의 모델 훈련 없이 프롬프트 조작만으로 구현 가능해 어떤 LLM에도 쉽게 적용할 수 있습니다.\\n*   **다재다능함:** 코딩, 작문, 추론 등 특정 분야에 국한되지 않고 다양한 종류의 과제에 효과적입니다.\\n*   **성능 향상:** 추가 데이터 없이 모델 자체의 잠재력을 끌어내 결과물의 질을 크게 높일 수 있습니다.\\n*   **해석 가능성:** '피드백' 단계 덕분에 모델이 왜 그렇게 답변을 수정했는지 그 이유를 알 수 있어 과정이 투명합니다.\\n\\n**단점:**\\n*   **비용 및 시간 증가:** 피드백과 개선을 위해 모델을 여러 번 호출해야 하므로, 시간과 비용(API 사용료)이 몇 배로 증가합니다.\\n*   **모델 의존성:** 모델 자체가 똑똑해야 효과가 있습니다. 성능이 낮은 모델은 스스로 유의미한 피드백을 만들지 못해 이 방법이 소용없습니다.\\n*   **지식의 한계:** 모델이 처음부터 잘못 알고 있는 사실은 스스로 교정하기 어렵습니다. 오히려 잘못된 믿음을 피드백 과정에서 더 강화할 위험도 있습니다.\", \"7. 이 논문에서 나온 추천할 만한 References.\\n이 논문의 아이디어는 LLM에게 생각의 과정을 단계별로 제시하게 하여 추론 능력을 높이는 연구들로부터 큰 영향을 받았습니다. 그중에서도 가장 대표적인 선행 연구는 바로 'Chain-of-Thought (CoT) Prompting'입니다. CoT는 복잡한 문제에 대해 '차근차근 생각해 보자(Let's think step by step)'라는 문구를 추가하는 것만으로 모델이 풀이 과정을 생성하게 하여 정답률을 높인 획기적인 연구입니다. Self-Refine은 이러한 '생각의 과정'을 단순히 나열하는 것을 넘어, 그 과정을 '평가'하고 '개선'하는 단계까지 확장했다는 점에서 CoT의 직계 후손이라고 볼 수 있습니다. 따라서 LLM의 추론 능력을 이해하는 데 있어 아래 논문을 함께 읽어보시길 강력히 추천합니다.\\n\\n*   **Wei, J., et al. (2022). Chain-of-thought prompting elicits reasoning in large language models.** 이 논문은 CoT 개념을 처음으로 제시하여 LLM 프롬프트 엔지니어링 분야에 큰 반향을 일으켰습니다.\", '8. 아래 정보도 개조식으로 정리해줘.\\n- title: Self-Refine: Iterative Refinement with Self-Feedback\\n- year: 2023\\n- citation: 599\\n- tags: Large Language Model, Prompt Engineering, In-Context Learning, Iterative Refinement', \"9. short_summary: 최종적으로 전체적인 내용을 핵심적인 2개 문장을 개조식으로 요약해줘.\\n- LLM이 생성한 초안을 스스로 피드백하고 수정하는 'Self-Refine' 프레임워크를 제안하여, 추가적인 학습 없이 다양한 태스크에서 결과물의 품질을 반복적으로 향상시킴.\\n- 이 방법은 여러 번의 API 호출로 비용과 시간이 증가하는 단점이 있지만, 별도의 훈련 데이터 없이도 모델의 내재된 능력을 최대한 활용하여 성능을 극대화하는 효과적인 접근법임.\"]\n",
      "9\n",
      "<class 'list'>\n",
      "✅ 페이지가 성공적으로 생성되었습니다!\n",
      "{'object': 'page', 'id': '2283a61e-46e5-815b-beba-c967e29ac7da', 'created_time': '2025-07-06T11:39:00.000Z', 'last_edited_time': '2025-07-06T11:39:00.000Z', 'created_by': {'object': 'user', 'id': '3de7c8f9-758d-465d-822e-3ca39da44e71'}, 'last_edited_by': {'object': 'user', 'id': '3de7c8f9-758d-465d-822e-3ca39da44e71'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '1763a61e-46e5-80c0-9b2d-c590f37618fd'}, 'archived': False, 'in_trash': False, 'properties': {'Citation': {'id': '%3AYEl', 'type': 'number', 'number': 599}, 'Done': {'id': 'JC%7Bo', 'type': 'button', 'button': {}}, 'URL': {'id': 'KGeq', 'type': 'url', 'url': 'https://arxiv.org/pdf/2305.07895'}, 'Created time': {'id': 'ZetJ', 'type': 'created_time', 'created_time': '2025-07-06T11:39:00.000Z'}, 'Year': {'id': '%5B%5CRf', 'type': 'number', 'number': 2023}, 'Summary': {'id': '%5Ew%5CF', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': \"- LLM이 생성한 초안을 스스로 피드백하고 수정하는 'Self-Refine' 프레임워크를 제안하여, 추가적인 학습 없이 다양한 태스크에서 결과물의 품질을 반복적으로 향상시킴.\\n- 이 방법은 여러 번의 API 호출로 비용과 시간이 증가하는 단점이 있지만, 별도의 훈련 데이터 없이도 모델의 내재된 능력을 최대한 활용하여 성능을 극대화하는 효과적인 접근법임.\", 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': \"- LLM이 생성한 초안을 스스로 피드백하고 수정하는 'Self-Refine' 프레임워크를 제안하여, 추가적인 학습 없이 다양한 태스크에서 결과물의 품질을 반복적으로 향상시킴.\\n- 이 방법은 여러 번의 API 호출로 비용과 시간이 증가하는 단점이 있지만, 별도의 훈련 데이터 없이도 모델의 내재된 능력을 최대한 활용하여 성능을 극대화하는 효과적인 접근법임.\", 'href': None}]}, 'End': {'id': 'bK%7Df', 'type': 'date', 'date': None}, 'Last edited time': {'id': 'gEAh', 'type': 'last_edited_time', 'last_edited_time': '2025-07-06T11:39:00.000Z'}, 'Tag': {'id': 'uOoM', 'type': 'multi_select', 'multi_select': [{'id': '3ffa3050-7bde-426e-ae3c-056680ea673b', 'name': 'Large Language Model', 'color': 'orange'}, {'id': '73a73b42-9307-4534-a5af-66ddffe6a3a1', 'name': 'prompt engineering', 'color': 'gray'}, {'id': '91e565b7-7431-4e02-9670-ecc1377f8f44', 'name': 'In-Context Learning', 'color': 'pink'}, {'id': '83303498-f750-484a-bee6-81157bd97c3a', 'name': 'Iterative Refinement', 'color': 'orange'}]}, 'Title': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Self-Refine: Iterative Refinement with Self-Feedback', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Self-Refine: Iterative Refinement with Self-Feedback', 'href': None}]}, 'Finish': {'id': 'fef1e3a8-9405-4839-a737-9e8c579f9d08', 'type': 'checkbox', 'checkbox': False}}, 'url': 'https://www.notion.so/Self-Refine-Iterative-Refinement-with-Self-Feedback-2283a61e46e5815bbebac967e29ac7da', 'public_url': None, 'request_id': 'c55722ba-ff19-4b85-836a-888c353bb545'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def parse_notion_json(output):\n",
    "    title = output.title\n",
    "    year = output.year\n",
    "    citation = output.citation\n",
    "    summary = output.short_summary\n",
    "    tags = output.tags\n",
    "    content = output.content\n",
    "    paper_url = output.paper_url\n",
    "\n",
    "    new_page_json = {\n",
    "        \"parent\": {\"database_id\": DATABASE_ID},\n",
    "        \"properties\": {\n",
    "            \"Title\": {  # ✅ Title은 반드시 있어야 함\n",
    "                \"title\": [{\"text\": {\"content\": title}}]\n",
    "            },\n",
    "            \"Year\": {\"number\": year},  # ✅ 숫자형\n",
    "            \"Citation\": {\"number\": citation},  # ✅ 숫자형\n",
    "            \"Summary\": {\"rich_text\": [{\"text\": {\"content\": summary}}]},  # ✅ rich_text\n",
    "            \"Tag\": {\"multi_select\": [{\"name\": tag} for tag in tags]},  # ✅ multi_select\n",
    "            \"Done\": {\"checkbox\": False},  # ✅ 체크박스\n",
    "            \"URL\": {\"url\": paper_url},  # ✅ URL\n",
    "            \"Finish\": {\"checkbox\": False},  # ✅ 체크박스\n",
    "            # \"End\": {\"date\": {\"start\": \"2025-07-06\"}},  # ✅ 날짜\n",
    "            \"End\": {\"date\": None},  # ✅ 날짜\n",
    "        },\n",
    "        \"children\": [  # ✅ 본문 내용 추가\n",
    "            {\n",
    "                \"object\": \"block\",\n",
    "                \"type\": \"heading_2\",\n",
    "                \"heading_2\": {\n",
    "                    \"rich_text\": [\n",
    "                        {\"type\": \"text\", \"text\": {\"content\": \"Paper Content\"}}\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"object\": \"block\",\n",
    "                \"type\": \"paragraph\",\n",
    "                \"paragraph\": {\n",
    "                    \"rich_text\": [\n",
    "                        {\"type\": \"text\", \"text\": {\"content\": content_item}}\n",
    "                        for content_item in content\n",
    "                    ]\n",
    "                },\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    return new_page_json\n",
    "\n",
    "\n",
    "# 환경변수 또는 직접 입력\n",
    "NOTION_API_KEY = os.getenv(\"NOTION_API_KEY\")  # 노션 Integration API 키\n",
    "DATABASE_ID = os.getenv(\"PAPER_DATABASE_ID\")  # 데이터베이스 ID\n",
    "NOTION_VERSION = \"2022-06-28\"\n",
    "\n",
    "\n",
    "# .env 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 환경변수 체크\n",
    "if not NOTION_API_KEY or not DATABASE_ID:\n",
    "    raise ValueError(\n",
    "        \"❌ NOTION_API_KEY 또는 PAPER_DATABASE_ID 환경변수가 비어있습니다.\"\n",
    "    )\n",
    "\n",
    "new_page_json = parse_notion_json(output)\n",
    "\n",
    "# API 요청\n",
    "response = requests.post(\n",
    "    \"https://api.notion.com/v1/pages\",\n",
    "    headers={\n",
    "        \"Authorization\": f\"Bearer {NOTION_API_KEY}\",\n",
    "        \"Notion-Version\": NOTION_VERSION,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    },\n",
    "    json=new_page_json,\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "if response.status_code in [200, 201]:\n",
    "    print(\"✅ 페이지가 성공적으로 생성되었습니다!\")\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(\"❌ 오류 발생:\", response.status_code)\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6931c4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834c726a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
